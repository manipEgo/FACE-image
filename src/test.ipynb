{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = \"../img/Outline/\"\n",
    "# Get the folder information\n",
    "info = load_folder_information(img_path)\n",
    "# Generate masks\n",
    "origin = load_image(os.path.join(img_path, info[\"origin\"]))\n",
    "\n",
    "print(info)\n",
    "plt.imshow(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_num = len(info[\"mask_prefix\"])\n",
    "masks = []\n",
    "for i in range(mask_num):\n",
    "    mask = np.zeros_like(origin)\n",
    "    masked_column = origin.shape[1] // (mask_num + 1) * (i + 1)\n",
    "    mask[:, masked_column:] = 1\n",
    "    masks.append(mask)\n",
    "\n",
    "# for line in range(masks[0].shape[0]):\n",
    "#     for column in range(masks[0].shape[1]):\n",
    "#         print(masks[0][line][column][0], end=\" \")\n",
    "#     print()\n",
    "plt.imshow(masks[6]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image files\n",
    "img_names = os.listdir(img_path)\n",
    "# MSE\n",
    "logits = []\n",
    "for i in range(mask_num):\n",
    "    mask = masks[i]\n",
    "    # for line in range(mask.shape[0]):\n",
    "    #     for column in range(mask.shape[1]):\n",
    "    #         print(mask[line][column][0], end=\" \")\n",
    "    #     print()\n",
    "    mask_prefix = info[\"mask_prefix\"][i]\n",
    "    imgs = [\n",
    "        load_image(os.path.join(img_path, img_name))\n",
    "        for img_name in img_names\n",
    "        if mask_prefix in img_name\n",
    "    ]\n",
    "    mse = [image_MSE(origin, img, mask) for img in imgs]\n",
    "    logits.append(np.mean(mse))\n",
    "    # for img in imgs:\n",
    "    #     plt.imshow(img)\n",
    "    #     plt.show()\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpd_path = \"/nas/Public/data/HPDv2/test/\"\n",
    "save_path = \"/home/lyy/Repos/FACE-image/data/\"\n",
    "\n",
    "import os\n",
    "\n",
    "file_list = []\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "for file in os.listdir(hpd_path):\n",
    "    os.symlink(os.path.join(hpd_path, file), os.path.join(save_path, f\"image-{int(file.split('.')[0])}.\" + file.split(\".\")[-1] if file.split(\".\")[-1] != \"\" else \"jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pygam import LinearGAM\n",
    "\n",
    "SPLIT = 64\n",
    "WIDTH = 512\n",
    "MODEL_NUMBER = 9\n",
    "\n",
    "hpd_path = \"/nas/Public/data/HPDv2/test/\"\n",
    "generate_path = \"/nas/Public/experiment_result/FACE-image/\"\n",
    "mapping_path = \"/nas/Public/data/HPDv2/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = json.load(open(mapping_path))\n",
    "files = os.listdir(generate_path)\n",
    "\n",
    "groups = []\n",
    "winners = []\n",
    "\n",
    "for item in mapping:\n",
    "    group_complete = True\n",
    "    if len(item[\"image_path\"]) != MODEL_NUMBER:\n",
    "        # cannot determine which models are used\n",
    "        continue\n",
    "    for path in item[\"image_path\"]:\n",
    "        # iterate image numbers\n",
    "        number = int(path.split('.')[0])\n",
    "        single_complete = True\n",
    "        for i in range(SPLIT-1):\n",
    "            # check each mask re-generation\n",
    "            if (f\"image-{number}_mask-{i}_00001_.png\") not in files:\n",
    "                single_complete = False\n",
    "                break\n",
    "        if not single_complete:\n",
    "            group_complete = False\n",
    "            break\n",
    "    if group_complete:\n",
    "        groups.append(item[\"image_path\"])\n",
    "        idx = 0\n",
    "        for i, rank in enumerate(item[\"rank\"]):\n",
    "            if rank == 0:\n",
    "                idx = i\n",
    "                break\n",
    "        winners.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups))\n",
    "print(groups[0])\n",
    "print(winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for names in groups:\n",
    "    for idx, name in enumerate(names):\n",
    "        for mask in range(SPLIT-1):\n",
    "            no = int(name.split('.')[0])\n",
    "            os.symlink(os.path.join(generate_path, f\"image-{no}_mask-{mask}_00001_.png\"), os.path.join(f\"../data/model_{idx}\", f\"image-{no}_mask-{mask}_00001_.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_list = []\n",
    "# for i in range(1, 3701):\n",
    "#     for j in range(SPLIT-1):\n",
    "#         if (f\"image-{i}_mask-{j}_00001_.png\") not in files:\n",
    "#             if i not in missing_list:\n",
    "#                 missing_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logits calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = {}\n",
    "\n",
    "# for i in range(MODEL_NUMBER):\n",
    "#     logits[i] = []\n",
    "\n",
    "# for group in tqdm(groups):\n",
    "#     for i, file in enumerate(group):\n",
    "#         subject_img = cv2.imread(os.path.join(generate_path, f\"image-{int(file.split('.')[0])}.jpg_00001_.png\"), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "#         idx = int(file.split('.')[0])\n",
    "#         # plt.imshow(subject_img)\n",
    "#         # plt.show()\n",
    "#         # print(idx)\n",
    "#         sequence = []\n",
    "#         # each mask strip\n",
    "#         for mask_idx in range(SPLIT - 1):\n",
    "#             generated_img = cv2.imread(os.path.join(generate_path, f\"image-{idx}_mask-{mask_idx}_00001_.png\"), cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "#             # one strip\n",
    "#             columns = WIDTH // SPLIT * (mask_idx + 1)\n",
    "#             token = np.array(generated_img[:, columns:columns + WIDTH // SPLIT], dtype=np.int16)\n",
    "#             ground_truth = np.array(subject_img[:, columns:columns + WIDTH // SPLIT], dtype=np.int16)\n",
    "\n",
    "#             mse = ((token - ground_truth) ** 2).mean()\n",
    "#             sequence.append(mse)\n",
    "#         logits[i].append(sequence)\n",
    "\n",
    "# with open(\"./logits.json\", \"w\") as f:\n",
    "#     json.dump(logits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./logits.json\", \"r\") as f:\n",
    "    logits = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ranking clac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_lists = {}\n",
    "rank_avgs = {}\n",
    "rank_var = {}\n",
    "for i in range(MODEL_NUMBER):\n",
    "    rank_lists[i] = []\n",
    "for item in mapping:\n",
    "    if len(item[\"rank\"]) != MODEL_NUMBER:\n",
    "        # cannot determine which models are used\n",
    "        continue\n",
    "    for i, rank in enumerate(item[\"rank\"]):\n",
    "        rank_lists[i].append(rank)\n",
    "\n",
    "for i in range(MODEL_NUMBER):\n",
    "    rank_avgs[i] = np.mean(rank_lists[i])\n",
    "    rank_var[i] = np.var(rank_lists[i])\n",
    "\n",
    "print(rank_avgs)\n",
    "print(rank_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_sum = []\n",
    "spectra = []\n",
    "for idx, winner in enumerate(winners):\n",
    "    logits_sum.append(logits[str(winner)][idx])\n",
    "    spectra.append(np.abs(np.fft.fft(logits[str(winner)][idx])))\n",
    "\n",
    "logits_winner_avg = np.mean(logits_sum, axis=0)\n",
    "spectrum_winner_avg = np.mean(spectra, axis=0)\n",
    "# print(logits_winner_avg)\n",
    "# print(spectrum_winner_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_logits = {}\n",
    "fig_size = (15, 10)\n",
    "\n",
    "plt.figure(0, fig_size)\n",
    "plt.plot(np.linspace(0, len(logits_winner_avg), len(logits_winner_avg)), logits_winner_avg, label=\"winner avg\", color=\"grey\")\n",
    "plt.legend()\n",
    "\n",
    "spectrum_winner_avg = spectrum_winner_avg[1:]\n",
    "\n",
    "plt.figure(1, fig_size)\n",
    "plt.plot(np.linspace(0, 0.5, len(spectrum_winner_avg)), spectrum_winner_avg, label=\"winner avg\", color=\"grey\")\n",
    "plt.legend()\n",
    "\n",
    "X = np.linspace(0, 0.5, len(spectrum_winner_avg))\n",
    "gam = LinearGAM().fit(X, spectrum_winner_avg)\n",
    "XX = gam.generate_X_grid(term=0, n=len(spectra))\n",
    "pdep, confi = gam.partial_dependence(term=0, X=XX, width=0.95)\n",
    "\n",
    "# XX =       XX[10:]\n",
    "# pdep =   pdep[10:]\n",
    "# confi = confi[10:]\n",
    "\n",
    "plt.figure(2, fig_size)\n",
    "plt.plot(\n",
    "    XX[:, 0],\n",
    "    np.abs(pdep),\n",
    "    label=\"winner avg\",\n",
    "    color=\"grey\"\n",
    ")\n",
    "# plt.fill_between(\n",
    "#     XX[:, 0],\n",
    "#     np.where(\n",
    "#         confi[:, 0] * confi[:, 1] > 0,\n",
    "#         np.minimum(np.abs(confi[:, 0]), np.abs(confi[:, 1])),\n",
    "#         0\n",
    "#     ),\n",
    "#     np.maximum(np.abs(confi[:, 0]), np.abs(confi[:, 1])),\n",
    "#     alpha=0.25\n",
    "# )\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "for i, logit_list in logits.items():\n",
    "    # print(len(logit_list[0]))\n",
    "    # break\n",
    "    avg_logits[i] = np.mean(logit_list, axis=0)\n",
    "\n",
    "    plt.figure(0, fig_size)\n",
    "    plt.title(\"Raw MSEs\")\n",
    "    plt.plot(np.linspace(0, len(avg_logits[i]), len(avg_logits[i])), avg_logits[i], label=f\"avg: {rank_avgs[int(i)]:.3}, var: {rank_var[int(i)]:.3}\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    spectra = []\n",
    "    for line in logit_list:\n",
    "        spectrum = np.abs(np.fft.fft(line))\n",
    "        spectra.append(spectrum)\n",
    "    mean_spectrum = np.mean(spectra, axis=0)\n",
    "\n",
    "    mean_spectrum = mean_spectrum[1:]\n",
    "\n",
    "    plt.figure(1, fig_size)\n",
    "    plt.title(\"Raw Spectra\")\n",
    "    plt.plot(np.linspace(0, 0.5, len(mean_spectrum)), mean_spectrum, label=f\"avg: {rank_avgs[int(i)]:.3}, var: {rank_var[int(i)]:.3}\")\n",
    "    plt.legend()\n",
    "\n",
    "    X = np.linspace(0, 0.5, len(mean_spectrum))\n",
    "    gam = LinearGAM().fit(X, mean_spectrum)\n",
    "    XX = gam.generate_X_grid(term=0, n=len(spectra))\n",
    "    pdep, confi = gam.partial_dependence(term=0, X=XX, width=0.95)\n",
    "\n",
    "    # XX =       XX[10:]\n",
    "    # pdep =   pdep[10:]\n",
    "    # confi = confi[10:]\n",
    "\n",
    "    plt.figure(2, fig_size)\n",
    "    plt.title(\"GAM Spectra\")\n",
    "    plt.plot(\n",
    "        XX[:, 0],\n",
    "        np.abs(pdep),\n",
    "        label=f\"avg: {rank_avgs[int(i)]:.3}, var: {rank_var[int(i)]:.3}\",\n",
    "    )\n",
    "    # plt.fill_between(\n",
    "    #     XX[:, 0],\n",
    "    #     np.where(\n",
    "    #         confi[:, 0] * confi[:, 1] > 0,\n",
    "    #         np.minimum(np.abs(confi[:, 0]), np.abs(confi[:, 1])),\n",
    "    #         0\n",
    "    #     ),\n",
    "    #     np.maximum(np.abs(confi[:, 0]), np.abs(confi[:, 1])),\n",
    "    #     alpha=0.25\n",
    "    # )\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def spectral_overlap(spectrum1, spectrum2):\n",
    "    spectra = np.concatenate([spectrum1[None], spectrum2[None]], axis=0)\n",
    "    return spectra.min(axis=0).sum() / spectra.max(axis=0).sum()\n",
    "\n",
    "def pearson_correlation(spectrum1, spectrum2):\n",
    "    return np.corrcoef(spectrum1, spectrum2)[0, 1]\n",
    "\n",
    "def earth_mover_distance(spectrum1, spectrum2):\n",
    "    p1 = [spectrum1[0]]\n",
    "    p2 = [spectrum2[0]]\n",
    "    for value in spectrum1[1:]:\n",
    "        p1.append(p1[-1] + value)\n",
    "    for value in spectrum2[1:]:\n",
    "        p2.append(p2[-1] + value)\n",
    "    p1 = np.array(p1)\n",
    "    p2 = np.array(p2)\n",
    "    p1 /= p1[-1]\n",
    "    p2 /= p2[-1]\n",
    "    return np.abs(p1 - p2).sum() / spectrum1.shape[0]\n",
    "\n",
    "def kl_divergence(spectrum1, spectrum2):\n",
    "    kl = entropy(abs(spectrum1), abs(spectrum2))\n",
    "    return kl\n",
    "\n",
    "def score(spectrum1, spectrum2):\n",
    "    so = spectral_overlap(spectrum1, spectrum2)\n",
    "    corr = pearson_correlation(spectrum1, spectrum2)\n",
    "    emd = earth_mover_distance(spectrum1, spectrum2)\n",
    "    kl = kl_divergence(spectrum1, spectrum2)\n",
    "    return so, corr, emd, kl\n",
    "\n",
    "def area_under_curve(spectrum):\n",
    "    return np.abs(spectrum).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_so_list = []\n",
    "mean_corr_list = []\n",
    "mean_emd_list = []\n",
    "mean_kl_list = []\n",
    "\n",
    "so_lists = []\n",
    "corr_lists = []\n",
    "emd_lists = []\n",
    "kl_lists = []\n",
    "\n",
    "for i, logit_list in logits.items():\n",
    "    # print(len(logit_list[0]))\n",
    "    # break\n",
    "    avg_logits[i] = np.mean(logit_list, axis=0)\n",
    "\n",
    "    so_list = []\n",
    "    corr_list = []\n",
    "    emd_list = []\n",
    "    kl_list = []\n",
    "\n",
    "    for line in logit_list:\n",
    "        spectrum = np.abs(np.fft.fft(line))\n",
    "        so, corr, emd, kl = score(spectrum_winner_avg, spectrum[1:])\n",
    "        so_list.append(so)\n",
    "        corr_list.append(corr)\n",
    "        emd_list.append(emd)\n",
    "        kl_list.append(kl)\n",
    "\n",
    "    so_lists.append(so_list)\n",
    "    corr_lists.append(corr_list)\n",
    "    emd_lists.append(emd_list)\n",
    "    kl_lists.append(kl_list)\n",
    "\n",
    "    # stepwise mean scores\n",
    "    mean_so = np.mean(so_list)\n",
    "    mean_corr = np.mean(corr_list)\n",
    "    mean_emd = np.mean(emd_list)\n",
    "    mean_kl = np.mean(kl_list)\n",
    "\n",
    "    mean_so_list.append(mean_so)\n",
    "    mean_corr_list.append(mean_corr)\n",
    "    mean_emd_list.append(mean_emd)\n",
    "    mean_kl_list.append(mean_kl)\n",
    "\n",
    "    print(f\"SO: {mean_so:.5}\\tCORR: {mean_corr:.5}\\tEMD: {mean_emd:.5}\\tKL: {mean_kl:.5}\")\n",
    "\n",
    "# stepwise rankings mean\n",
    "so_ranks_idx = np.argsort(-np.array(so_lists), axis=0)\n",
    "corr_ranks_idx = np.argsort(-np.array(corr_lists), axis=0)\n",
    "emd_ranks_idx = np.argsort(np.array(emd_lists), axis=0)\n",
    "kl_ranks_idx = np.argsort(np.array(kl_lists), axis=0)\n",
    "so_ranks = np.zeros(so_ranks_idx.shape)\n",
    "corr_ranks = np.zeros(corr_ranks_idx.shape)\n",
    "emd_ranks = np.zeros(emd_ranks_idx.shape)\n",
    "kl_ranks = np.zeros(kl_ranks_idx.shape)\n",
    "for j in range(so_ranks_idx.shape[1]):\n",
    "    for i in range(so_ranks_idx.shape[0]):\n",
    "        so_ranks[so_ranks_idx[i][j]][j] = i\n",
    "        corr_ranks[corr_ranks_idx[i][j]][j] = i\n",
    "        emd_ranks[emd_ranks_idx[i][j]][j] = i\n",
    "        kl_ranks[kl_ranks_idx[i][j]][j] = i\n",
    "\n",
    "print('='*10 + \" stepwise rankings \" + '='*10)\n",
    "\n",
    "for i in range(so_ranks.shape[0]):\n",
    "    print(f\"AVG: {(np.mean(so_ranks[i])+np.mean(corr_ranks[i])+np.mean(emd_ranks[i])+np.mean(kl_ranks[i]))/4:.3}\\tSO: {np.mean(so_ranks[i]):.3}\\tCORR: {np.mean(corr_ranks[i]):.3}\\tEMD: {np.mean(emd_ranks[i]):.3}\\tKL: {np.mean(kl_ranks[i]):.3}\")\n",
    "\n",
    "# mean scores ranking\n",
    "so_ranks_idx = np.argsort(-np.array(mean_so_list))\n",
    "corr_ranks_idx = np.argsort(-np.array(mean_corr_list))\n",
    "emd_ranks_idx = np.argsort(mean_emd_list)\n",
    "kl_ranks_idx = np.argsort(mean_kl_list)\n",
    "so_ranks = np.zeros((len(so_ranks_idx)))\n",
    "corr_ranks = np.zeros((len(corr_ranks_idx)))\n",
    "emd_ranks = np.zeros((len(emd_ranks_idx)))\n",
    "kl_ranks = np.zeros((len(kl_ranks_idx)))\n",
    "for i in range(len(so_ranks_idx)):\n",
    "    so_ranks[so_ranks_idx[i]] = i\n",
    "    corr_ranks[corr_ranks_idx[i]] = i\n",
    "    emd_ranks[emd_ranks_idx[i]] = i\n",
    "    kl_ranks[kl_ranks_idx[i]] = i\n",
    "\n",
    "print('='*10 + \" rankings \" + '='*10)\n",
    "\n",
    "for i in range(len(so_ranks)):\n",
    "    print(f\"AVG: {(so_ranks[i]+corr_ranks[i]+emd_ranks[i]+kl_ranks[i])/4}\\tSO: {so_ranks[i]}\\tCORR: {corr_ranks[i]}\\tEMD: {emd_ranks[i]}\\tKL: {kl_ranks[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inception score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from torchvision.models.inception import inception_v3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# we should use same mean and std for inception v3 model in training and testing process\n",
    "# reference web page: https://pytorch.org/hub/pytorch_vision_inception_v3/\n",
    "\n",
    "def imread(filename):\n",
    "    \"\"\"\n",
    "    Loads an image file into a (height, width, 3) uint8 ndarray.\n",
    "    \"\"\"\n",
    "    return np.asarray(Image.open(filename), dtype=np.uint8)[..., :3]\n",
    "\n",
    "def inception_score(batch_size=50, resize=False, splits=10):\n",
    "    # Set up dtype\n",
    "    device = torch.device(\"cuda:1\")  # you can change the index of cuda\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='nearest', align_corners=False).to(device)\n",
    "    \n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions using pre-trained inception_v3 model\n",
    "    print('Computing predictions using inception v3 model')\n",
    "    \n",
    "\n",
    "    files = readDir()\n",
    "    N = len(files)\n",
    "    preds = np.zeros((N, 1000))\n",
    "    if batch_size > N:\n",
    "        print(('Warning: batch size is bigger than the data size. '\n",
    "                 'Setting batch size to data size'))\n",
    "\n",
    "    for i in tqdm(range(0, N, batch_size)):\n",
    "        start = i\n",
    "        end = i + batch_size\n",
    "        images = np.array([imread(str(f)).astype(np.float32)\n",
    "                           for f in files[start:end]])\n",
    "\n",
    "        # Reshape to (n_images, 3, height, width)\n",
    "        images = images.transpose((0, 3, 1, 2))\n",
    "        images /= 255\n",
    "\n",
    "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        batch = batch.to(device)\n",
    "        y = get_pred(batch)\n",
    "        # print(y.shape)\n",
    "        preds[i :i  + batch_size] = get_pred(batch)\n",
    "        \n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Now compute the mean KL Divergence\n",
    "    print('Computing KL Divergence')\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k + 1) * (N // splits), :] # split the whole data into several parts\n",
    "        py = np.mean(part, axis=0)  # marginal probability\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]  # conditional probability\n",
    "            scores.append(entropy(pyx, py))  # compute divergence\n",
    "        split_scores.append(np.exp(scores))\n",
    "\n",
    "    return np.max(split_scores), np.mean(split_scores)\n",
    "\n",
    "def readDir():\n",
    "    dirPath = \"/home/lyy/Repos/FACE-image/data/model_8\"\n",
    "    allFiles = []\n",
    "    if os.path.isdir(dirPath):\n",
    "        fileList = os.listdir(dirPath)\n",
    "        for f in fileList:\n",
    "            f = dirPath+'/'+f\n",
    "            if os.path.isdir(f):\n",
    "                subFiles = readDir(f)\n",
    "                allFiles = subFiles + allFiles\n",
    "            else:\n",
    "                allFiles.append(f)\n",
    "        return allFiles\n",
    "    else:\n",
    "        return 'Error,not a dir'\n",
    "\n",
    "MAX, IS= inception_score(splits=10)\n",
    "print('MAX IS is %.4f' % MAX)\n",
    "print('The IS is %.4f' % IS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
